{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lamhita/Spoon-Knife/blob/master/homeworks/hw02_attention_and_language_modeling/hw02_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Hc099wN9U3"
      },
      "source": [
        "### Генерация поэзии с помощью нейронных сетей: шаг 1\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev\n",
        "\n",
        "Ваша основная задача: научиться генерироват стихи с помощью простой рекуррентной нейронной сети (Vanilla RNN). В качестве корпуса текстов для обучения будет выступать роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T9rY5kEHN9U5"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "import string\n",
        "import os\n",
        "from random import sample\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ibnO4CrN9U5",
        "outputId": "03bfc2a8-82b6-42e4-82d5-c547d546b6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda device is available\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print('{} device is available'.format(device))\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPenWOy01Ooa",
        "outputId": "a92e8e33-e009-4bd4-ac12-3b1b5e1cd3f2"
      },
      "source": [
        "#### 1. Загрузка данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO4H7sgSN9U6",
        "outputId": "7f20dd7f-8a4b-4ee7-e346-007b9b3a6b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-03 13:30:51--  https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 262521 (256K) [text/plain]\n",
            "Saving to: ‘onegin.txt’\n",
            "\n",
            "\ronegin.txt            0%[                    ]       0  --.-KB/s               \ronegin.txt          100%[===================>] 256.37K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-11-03 13:30:51 (10.1 MB/s) - ‘onegin.txt’ saved [262521/262521]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "!wget https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/onegin.txt\n",
        "\n",
        "with open('onegin.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "\n",
        "text = \"\".join([x.replace('\\t\\t', '').lower() for x in text])\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQYpmGfR_gJ8"
      },
      "source": [
        "#### 2. Построение словаря и предобработка текста\n",
        "В данном задании требуется построить языковую модель на уровне символов. Приведем весь текст к нижнему регистру и построим словарь из всех символов в доступном корпусе текстов. Также добавим токен `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJbE8D3MN9U6",
        "outputId": "5f2b4fc6-1e25-46ec-f2ff-30e39bad2618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems fine!\n"
          ]
        }
      ],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "tokens = sorted(set(text.lower())) + ['<sos>']\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "assert num_tokens == 84, \"Check the tokenization process\"\n",
        "\n",
        "token_to_idx = {x: idx for idx, x in enumerate(tokens)}\n",
        "idx_to_token = {idx: x for idx, x in enumerate(tokens)}\n",
        "\n",
        "assert len(tokens) == len(token_to_idx), \"Mapping should be unique\"\n",
        "\n",
        "print(\"Seems fine!\")\n",
        "vocab_size = 84\n",
        "\n",
        "text_encoded = [token_to_idx[x] for x in text]\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtM2c2MoN9U6"
      },
      "source": [
        "__Ваша задача__: обучить классическую рекуррентную нейронную сеть (Vanilla RNN) предсказывать следующий символ на полученном корпусе текстов и сгенерировать последовательность длины 100 для фиксированной начальной фразы.\n",
        "\n",
        "Вы можете воспользоваться кодом с занятие №6 или же обратиться к следующим ссылкам:\n",
        "* Замечательная статья за авторством Andrej Karpathy об использовании RNN: [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "* Пример char-rnn от Andrej Karpathy: [github repo](https://github.com/karpathy/char-rnn)\n",
        "* Замечательный пример генерации поэзии Шекспира: [github repo](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)\n",
        "\n",
        "Данное задание является достаточно творческим. Не страшно, если поначалу оно вызывает затруднения. Последняя ссылка в списке выше может быть особенно полезна в данном случае.\n",
        "\n",
        "Далее для вашего удобства реализована функция, которая генерирует случайный батч размера `batch_size` из строк длиной `seq_length`. Вы можете использовать его при обучении модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lOeSi9xfN9U6"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "batch_size = 256\n",
        "seq_length = 100\n",
        "start_column = np.zeros((batch_size, 1), dtype=int) + token_to_idx['<sos>']\n",
        "\n",
        "def generate_chunk():\n",
        "    global text_encoded, start_column, batch_size, seq_length\n",
        "\n",
        "    start_index = np.random.randint(0, len(text_encoded) - batch_size*seq_length - 1)\n",
        "    data = np.array(text_encoded[start_index:start_index + batch_size*seq_length]).reshape((batch_size, -1))\n",
        "    yield np.hstack((start_column, data))\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnoFZghGN9U6"
      },
      "source": [
        "Пример батча:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHw9i0_LN9U6",
        "outputId": "c7170cc6-6e56-4996-c24e-8e89aac65fb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[83, 76, 63, ..., 61, 59, 48],\n",
              "       [83, 45,  5, ...,  1, 74, 63],\n",
              "       [83, 59,  1, ...,  1, 61, 45],\n",
              "       ...,\n",
              "       [83, 72, 50, ..., 47, 69, 53],\n",
              "       [83, 66,  1, ..., 56,  1, 59],\n",
              "       [83, 63,  1, ..., 59, 57,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "next(generate_chunk())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPLEuZExN9U7"
      },
      "source": [
        "Далее вам предстоит написать код для обучения модели и генерации текста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0alqcBV0N9U7"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Vanilla RNN layer\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Fully connected output layer\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize hidden state with zeros\n",
        "        return torch.zeros(1, batch_size, self.hidden_dim).to(device)\n",
        "\n",
        "# Instantiate model, define loss and optimizer\n",
        "model = CharRNN(len(tokens), embedding_dim=128, hidden_dim=256).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNSpOB_8N9U8"
      },
      "source": [
        "В качестве иллюстрации ниже доступен график значений функции потерь, построенный в ходе обучения авторской сети (сам код для ее обучения вам и предстоит написать)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bk_xOioN9U8",
        "outputId": "5aa8c277-b0dc-4d4f-c0cd-6408b94ba6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 2.3314990997314453\n",
            "Epoch [2/1000], Loss: 2.3271734714508057\n",
            "Epoch [3/1000], Loss: 2.303549289703369\n",
            "Epoch [4/1000], Loss: 2.3230319023132324\n",
            "Epoch [5/1000], Loss: 2.3172080516815186\n",
            "Epoch [6/1000], Loss: 2.3371379375457764\n",
            "Epoch [7/1000], Loss: 2.3390493392944336\n",
            "Epoch [8/1000], Loss: 2.3390488624572754\n",
            "Epoch [9/1000], Loss: 2.3210561275482178\n",
            "Epoch [10/1000], Loss: 2.3340542316436768\n",
            "Epoch [11/1000], Loss: 2.3044686317443848\n",
            "Epoch [12/1000], Loss: 2.3222601413726807\n",
            "Epoch [13/1000], Loss: 2.2959465980529785\n",
            "Epoch [14/1000], Loss: 2.283808708190918\n",
            "Epoch [15/1000], Loss: 2.2966103553771973\n",
            "Epoch [16/1000], Loss: 2.3051064014434814\n",
            "Epoch [17/1000], Loss: 2.317984104156494\n",
            "Epoch [18/1000], Loss: 2.2932584285736084\n",
            "Epoch [19/1000], Loss: 2.2897214889526367\n",
            "Epoch [20/1000], Loss: 2.3104708194732666\n",
            "Epoch [21/1000], Loss: 2.293152093887329\n",
            "Epoch [22/1000], Loss: 2.2683513164520264\n",
            "Epoch [23/1000], Loss: 2.285087823867798\n",
            "Epoch [24/1000], Loss: 2.2605926990509033\n",
            "Epoch [25/1000], Loss: 2.2728652954101562\n",
            "Epoch [26/1000], Loss: 2.2790608406066895\n",
            "Epoch [27/1000], Loss: 2.285548686981201\n",
            "Epoch [28/1000], Loss: 2.266104221343994\n",
            "Epoch [29/1000], Loss: 2.271064519882202\n",
            "Epoch [30/1000], Loss: 2.2526910305023193\n",
            "Epoch [31/1000], Loss: 2.2577626705169678\n",
            "Epoch [32/1000], Loss: 2.2800850868225098\n",
            "Epoch [33/1000], Loss: 2.251160144805908\n",
            "Epoch [34/1000], Loss: 2.2942206859588623\n",
            "Epoch [35/1000], Loss: 2.239889621734619\n",
            "Epoch [36/1000], Loss: 2.2807695865631104\n",
            "Epoch [37/1000], Loss: 2.241652488708496\n",
            "Epoch [38/1000], Loss: 2.278719425201416\n",
            "Epoch [39/1000], Loss: 2.2695212364196777\n",
            "Epoch [40/1000], Loss: 2.2498326301574707\n",
            "Epoch [41/1000], Loss: 2.233654737472534\n",
            "Epoch [42/1000], Loss: 2.243138313293457\n",
            "Epoch [43/1000], Loss: 2.2424657344818115\n",
            "Epoch [44/1000], Loss: 2.2347404956817627\n",
            "Epoch [45/1000], Loss: 2.2381064891815186\n",
            "Epoch [46/1000], Loss: 2.234307050704956\n",
            "Epoch [47/1000], Loss: 2.225656032562256\n",
            "Epoch [48/1000], Loss: 2.216871976852417\n",
            "Epoch [49/1000], Loss: 2.222238779067993\n",
            "Epoch [50/1000], Loss: 2.2198643684387207\n",
            "Epoch [51/1000], Loss: 2.204040050506592\n",
            "Epoch [52/1000], Loss: 2.239511013031006\n",
            "Epoch [53/1000], Loss: 2.2357146739959717\n",
            "Epoch [54/1000], Loss: 2.2490487098693848\n",
            "Epoch [55/1000], Loss: 2.2165870666503906\n",
            "Epoch [56/1000], Loss: 2.2216973304748535\n",
            "Epoch [57/1000], Loss: 2.203207492828369\n",
            "Epoch [58/1000], Loss: 2.200634002685547\n",
            "Epoch [59/1000], Loss: 2.2316932678222656\n",
            "Epoch [60/1000], Loss: 2.208836555480957\n",
            "Epoch [61/1000], Loss: 2.196234703063965\n",
            "Epoch [62/1000], Loss: 2.1903390884399414\n",
            "Epoch [63/1000], Loss: 2.196606397628784\n",
            "Epoch [64/1000], Loss: 2.218290090560913\n",
            "Epoch [65/1000], Loss: 2.1933062076568604\n",
            "Epoch [66/1000], Loss: 2.188450336456299\n",
            "Epoch [67/1000], Loss: 2.173922538757324\n",
            "Epoch [68/1000], Loss: 2.2132585048675537\n",
            "Epoch [69/1000], Loss: 2.2222681045532227\n",
            "Epoch [70/1000], Loss: 2.221822738647461\n",
            "Epoch [71/1000], Loss: 2.207592010498047\n",
            "Epoch [72/1000], Loss: 2.17842435836792\n",
            "Epoch [73/1000], Loss: 2.2133629322052\n",
            "Epoch [74/1000], Loss: 2.2092370986938477\n",
            "Epoch [75/1000], Loss: 2.2239065170288086\n",
            "Epoch [76/1000], Loss: 2.2248473167419434\n",
            "Epoch [77/1000], Loss: 2.210261821746826\n",
            "Epoch [78/1000], Loss: 2.187995195388794\n",
            "Epoch [79/1000], Loss: 2.196197271347046\n",
            "Epoch [80/1000], Loss: 2.1879701614379883\n",
            "Epoch [81/1000], Loss: 2.169757604598999\n",
            "Epoch [82/1000], Loss: 2.205333948135376\n",
            "Epoch [83/1000], Loss: 2.1579673290252686\n",
            "Epoch [84/1000], Loss: 2.1608500480651855\n",
            "Epoch [85/1000], Loss: 2.1627278327941895\n",
            "Epoch [86/1000], Loss: 2.160738945007324\n",
            "Epoch [87/1000], Loss: 2.171895742416382\n",
            "Epoch [88/1000], Loss: 2.181016445159912\n",
            "Epoch [89/1000], Loss: 2.1915318965911865\n",
            "Epoch [90/1000], Loss: 2.1601479053497314\n",
            "Epoch [91/1000], Loss: 2.1978065967559814\n",
            "Epoch [92/1000], Loss: 2.1300432682037354\n",
            "Epoch [93/1000], Loss: 2.1332671642303467\n",
            "Epoch [94/1000], Loss: 2.137545108795166\n",
            "Epoch [95/1000], Loss: 2.1811814308166504\n",
            "Epoch [96/1000], Loss: 2.13491153717041\n",
            "Epoch [97/1000], Loss: 2.165825128555298\n",
            "Epoch [98/1000], Loss: 2.121103048324585\n",
            "Epoch [99/1000], Loss: 2.156346321105957\n",
            "Epoch [100/1000], Loss: 2.123462200164795\n",
            "Epoch [101/1000], Loss: 2.1341121196746826\n",
            "Epoch [102/1000], Loss: 2.1302711963653564\n",
            "Epoch [103/1000], Loss: 2.1321654319763184\n",
            "Epoch [104/1000], Loss: 2.1243066787719727\n",
            "Epoch [105/1000], Loss: 2.1645967960357666\n",
            "Epoch [106/1000], Loss: 2.122746229171753\n",
            "Epoch [107/1000], Loss: 2.164356231689453\n",
            "Epoch [108/1000], Loss: 2.1620349884033203\n",
            "Epoch [109/1000], Loss: 2.1432414054870605\n",
            "Epoch [110/1000], Loss: 2.1087968349456787\n",
            "Epoch [111/1000], Loss: 2.1268956661224365\n",
            "Epoch [112/1000], Loss: 2.1062676906585693\n",
            "Epoch [113/1000], Loss: 2.1373326778411865\n",
            "Epoch [114/1000], Loss: 2.1410787105560303\n",
            "Epoch [115/1000], Loss: 2.1099233627319336\n",
            "Epoch [116/1000], Loss: 2.094052314758301\n",
            "Epoch [117/1000], Loss: 2.116262912750244\n",
            "Epoch [118/1000], Loss: 2.1726605892181396\n",
            "Epoch [119/1000], Loss: 2.1459436416625977\n",
            "Epoch [120/1000], Loss: 2.123352289199829\n",
            "Epoch [121/1000], Loss: 2.141221761703491\n",
            "Epoch [122/1000], Loss: 2.138680934906006\n",
            "Epoch [123/1000], Loss: 2.1363608837127686\n",
            "Epoch [124/1000], Loss: 2.122589111328125\n",
            "Epoch [125/1000], Loss: 2.1005032062530518\n",
            "Epoch [126/1000], Loss: 2.0967607498168945\n",
            "Epoch [127/1000], Loss: 2.1036548614501953\n",
            "Epoch [128/1000], Loss: 2.125176429748535\n",
            "Epoch [129/1000], Loss: 2.1064984798431396\n",
            "Epoch [130/1000], Loss: 2.1143407821655273\n",
            "Epoch [131/1000], Loss: 2.0900943279266357\n",
            "Epoch [132/1000], Loss: 2.1126742362976074\n",
            "Epoch [133/1000], Loss: 2.131237268447876\n",
            "Epoch [134/1000], Loss: 2.1172828674316406\n",
            "Epoch [135/1000], Loss: 2.0714046955108643\n",
            "Epoch [136/1000], Loss: 2.0807411670684814\n",
            "Epoch [137/1000], Loss: 2.105856418609619\n",
            "Epoch [138/1000], Loss: 2.0798346996307373\n",
            "Epoch [139/1000], Loss: 2.096931219100952\n",
            "Epoch [140/1000], Loss: 2.0763726234436035\n",
            "Epoch [141/1000], Loss: 2.060918092727661\n",
            "Epoch [142/1000], Loss: 2.0634703636169434\n",
            "Epoch [143/1000], Loss: 2.0854339599609375\n",
            "Epoch [144/1000], Loss: 2.069124698638916\n",
            "Epoch [145/1000], Loss: 2.0583252906799316\n",
            "Epoch [146/1000], Loss: 2.0458109378814697\n",
            "Epoch [147/1000], Loss: 2.0871095657348633\n",
            "Epoch [148/1000], Loss: 2.041104793548584\n",
            "Epoch [149/1000], Loss: 2.069403648376465\n",
            "Epoch [150/1000], Loss: 2.133425235748291\n",
            "Epoch [151/1000], Loss: 2.0686445236206055\n",
            "Epoch [152/1000], Loss: 2.1099493503570557\n",
            "Epoch [153/1000], Loss: 2.0586843490600586\n",
            "Epoch [154/1000], Loss: 2.081111431121826\n",
            "Epoch [155/1000], Loss: 2.113664388656616\n",
            "Epoch [156/1000], Loss: 2.083460807800293\n",
            "Epoch [157/1000], Loss: 2.1243152618408203\n",
            "Epoch [158/1000], Loss: 2.0654420852661133\n",
            "Epoch [159/1000], Loss: 2.0625553131103516\n",
            "Epoch [160/1000], Loss: 2.088130474090576\n",
            "Epoch [161/1000], Loss: 2.049496650695801\n",
            "Epoch [162/1000], Loss: 2.124584436416626\n",
            "Epoch [163/1000], Loss: 2.0574605464935303\n",
            "Epoch [164/1000], Loss: 2.113197088241577\n",
            "Epoch [165/1000], Loss: 2.074570417404175\n",
            "Epoch [166/1000], Loss: 2.0883443355560303\n",
            "Epoch [167/1000], Loss: 2.0457100868225098\n",
            "Epoch [168/1000], Loss: 2.0955464839935303\n",
            "Epoch [169/1000], Loss: 2.112250566482544\n",
            "Epoch [170/1000], Loss: 2.0460212230682373\n",
            "Epoch [171/1000], Loss: 2.0969884395599365\n",
            "Epoch [172/1000], Loss: 2.105056047439575\n",
            "Epoch [173/1000], Loss: 2.0591139793395996\n",
            "Epoch [174/1000], Loss: 2.0566320419311523\n",
            "Epoch [175/1000], Loss: 2.0598413944244385\n",
            "Epoch [176/1000], Loss: 2.025818347930908\n",
            "Epoch [177/1000], Loss: 2.047227382659912\n",
            "Epoch [178/1000], Loss: 2.081819772720337\n",
            "Epoch [179/1000], Loss: 2.066657543182373\n",
            "Epoch [180/1000], Loss: 2.03330397605896\n",
            "Epoch [181/1000], Loss: 2.044532060623169\n",
            "Epoch [182/1000], Loss: 2.0340933799743652\n",
            "Epoch [183/1000], Loss: 2.0420944690704346\n",
            "Epoch [184/1000], Loss: 2.055379867553711\n",
            "Epoch [185/1000], Loss: 2.016057014465332\n",
            "Epoch [186/1000], Loss: 2.075347661972046\n",
            "Epoch [187/1000], Loss: 2.055389404296875\n",
            "Epoch [188/1000], Loss: 2.046114206314087\n",
            "Epoch [189/1000], Loss: 2.026916742324829\n",
            "Epoch [190/1000], Loss: 2.0529353618621826\n",
            "Epoch [191/1000], Loss: 2.0875186920166016\n",
            "Epoch [192/1000], Loss: 2.0244979858398438\n",
            "Epoch [193/1000], Loss: 2.0000436305999756\n",
            "Epoch [194/1000], Loss: 2.0089755058288574\n",
            "Epoch [195/1000], Loss: 2.0569984912872314\n",
            "Epoch [196/1000], Loss: 2.0672719478607178\n",
            "Epoch [197/1000], Loss: 2.009542942047119\n",
            "Epoch [198/1000], Loss: 1.9988316297531128\n",
            "Epoch [199/1000], Loss: 2.021806240081787\n",
            "Epoch [200/1000], Loss: 2.015803575515747\n",
            "Epoch [201/1000], Loss: 2.0065324306488037\n",
            "Epoch [202/1000], Loss: 1.997189998626709\n",
            "Epoch [203/1000], Loss: 2.0523078441619873\n",
            "Epoch [204/1000], Loss: 2.018782138824463\n",
            "Epoch [205/1000], Loss: 1.9801446199417114\n",
            "Epoch [206/1000], Loss: 2.060305118560791\n",
            "Epoch [207/1000], Loss: 1.980119228363037\n",
            "Epoch [208/1000], Loss: 2.0054399967193604\n",
            "Epoch [209/1000], Loss: 1.9956505298614502\n",
            "Epoch [210/1000], Loss: 2.0170693397521973\n",
            "Epoch [211/1000], Loss: 2.038045883178711\n",
            "Epoch [212/1000], Loss: 1.9908486604690552\n",
            "Epoch [213/1000], Loss: 2.022195816040039\n",
            "Epoch [214/1000], Loss: 1.981868863105774\n",
            "Epoch [215/1000], Loss: 2.0341548919677734\n",
            "Epoch [216/1000], Loss: 2.040794610977173\n",
            "Epoch [217/1000], Loss: 2.020339012145996\n",
            "Epoch [218/1000], Loss: 1.992795467376709\n",
            "Epoch [219/1000], Loss: 2.0123870372772217\n",
            "Epoch [220/1000], Loss: 2.0398130416870117\n",
            "Epoch [221/1000], Loss: 1.9629946947097778\n",
            "Epoch [222/1000], Loss: 1.9617969989776611\n",
            "Epoch [223/1000], Loss: 1.9971896409988403\n",
            "Epoch [224/1000], Loss: 1.9760360717773438\n",
            "Epoch [225/1000], Loss: 2.0300238132476807\n",
            "Epoch [226/1000], Loss: 1.9938870668411255\n",
            "Epoch [227/1000], Loss: 1.9693337678909302\n",
            "Epoch [228/1000], Loss: 1.9730477333068848\n",
            "Epoch [229/1000], Loss: 1.9452948570251465\n",
            "Epoch [230/1000], Loss: 1.954890489578247\n",
            "Epoch [231/1000], Loss: 1.9457292556762695\n",
            "Epoch [232/1000], Loss: 1.9843806028366089\n",
            "Epoch [233/1000], Loss: 2.0011487007141113\n",
            "Epoch [234/1000], Loss: 1.9957149028778076\n",
            "Epoch [235/1000], Loss: 1.9814578294754028\n",
            "Epoch [236/1000], Loss: 1.9566354751586914\n",
            "Epoch [237/1000], Loss: 2.009392499923706\n",
            "Epoch [238/1000], Loss: 1.9541938304901123\n",
            "Epoch [239/1000], Loss: 2.0064496994018555\n",
            "Epoch [240/1000], Loss: 2.018795967102051\n",
            "Epoch [241/1000], Loss: 1.9621479511260986\n",
            "Epoch [242/1000], Loss: 2.0185158252716064\n",
            "Epoch [243/1000], Loss: 1.943308711051941\n",
            "Epoch [244/1000], Loss: 1.964037299156189\n",
            "Epoch [245/1000], Loss: 1.9433289766311646\n",
            "Epoch [246/1000], Loss: 2.0298476219177246\n",
            "Epoch [247/1000], Loss: 2.011030673980713\n",
            "Epoch [248/1000], Loss: 1.9692327976226807\n",
            "Epoch [249/1000], Loss: 1.9308308362960815\n",
            "Epoch [250/1000], Loss: 1.9429103136062622\n",
            "Epoch [251/1000], Loss: 1.9487956762313843\n",
            "Epoch [252/1000], Loss: 2.0001060962677\n",
            "Epoch [253/1000], Loss: 1.939590334892273\n",
            "Epoch [254/1000], Loss: 1.9652351140975952\n",
            "Epoch [255/1000], Loss: 1.9124537706375122\n",
            "Epoch [256/1000], Loss: 1.9749548435211182\n",
            "Epoch [257/1000], Loss: 1.9082155227661133\n",
            "Epoch [258/1000], Loss: 1.9749354124069214\n",
            "Epoch [259/1000], Loss: 1.9198882579803467\n",
            "Epoch [260/1000], Loss: 1.9652743339538574\n",
            "Epoch [261/1000], Loss: 2.0163588523864746\n",
            "Epoch [262/1000], Loss: 2.014854907989502\n",
            "Epoch [263/1000], Loss: 1.906877875328064\n",
            "Epoch [264/1000], Loss: 1.9743767976760864\n",
            "Epoch [265/1000], Loss: 1.922776222229004\n",
            "Epoch [266/1000], Loss: 1.9370341300964355\n",
            "Epoch [267/1000], Loss: 1.9412919282913208\n",
            "Epoch [268/1000], Loss: 1.9375033378601074\n",
            "Epoch [269/1000], Loss: 1.9862916469573975\n",
            "Epoch [270/1000], Loss: 1.9266972541809082\n",
            "Epoch [271/1000], Loss: 1.9497452974319458\n",
            "Epoch [272/1000], Loss: 1.9205806255340576\n",
            "Epoch [273/1000], Loss: 1.9586628675460815\n",
            "Epoch [274/1000], Loss: 1.970955491065979\n",
            "Epoch [275/1000], Loss: 1.9288607835769653\n",
            "Epoch [276/1000], Loss: 2.0114612579345703\n",
            "Epoch [277/1000], Loss: 1.8988893032073975\n",
            "Epoch [278/1000], Loss: 1.9841675758361816\n",
            "Epoch [279/1000], Loss: 1.9988614320755005\n",
            "Epoch [280/1000], Loss: 1.9704327583312988\n",
            "Epoch [281/1000], Loss: 1.9706056118011475\n",
            "Epoch [282/1000], Loss: 1.9610912799835205\n",
            "Epoch [283/1000], Loss: 1.9571876525878906\n",
            "Epoch [284/1000], Loss: 1.9392770528793335\n",
            "Epoch [285/1000], Loss: 1.9612746238708496\n",
            "Epoch [286/1000], Loss: 1.9745668172836304\n",
            "Epoch [287/1000], Loss: 1.9446693658828735\n",
            "Epoch [288/1000], Loss: 1.968483328819275\n",
            "Epoch [289/1000], Loss: 1.9184952974319458\n",
            "Epoch [290/1000], Loss: 1.9378231763839722\n",
            "Epoch [291/1000], Loss: 1.9372442960739136\n",
            "Epoch [292/1000], Loss: 1.910762906074524\n",
            "Epoch [293/1000], Loss: 1.9064372777938843\n",
            "Epoch [294/1000], Loss: 1.9265252351760864\n",
            "Epoch [295/1000], Loss: 1.9462308883666992\n",
            "Epoch [296/1000], Loss: 1.9288482666015625\n",
            "Epoch [297/1000], Loss: 1.9290003776550293\n",
            "Epoch [298/1000], Loss: 1.9283627271652222\n",
            "Epoch [299/1000], Loss: 1.9235550165176392\n",
            "Epoch [300/1000], Loss: 1.8988964557647705\n",
            "Epoch [301/1000], Loss: 1.886869192123413\n",
            "Epoch [302/1000], Loss: 1.8924463987350464\n",
            "Epoch [303/1000], Loss: 1.9031176567077637\n",
            "Epoch [304/1000], Loss: 1.9275023937225342\n",
            "Epoch [305/1000], Loss: 1.9009708166122437\n",
            "Epoch [306/1000], Loss: 1.9673511981964111\n",
            "Epoch [307/1000], Loss: 1.9510560035705566\n",
            "Epoch [308/1000], Loss: 1.9729902744293213\n",
            "Epoch [309/1000], Loss: 1.9139914512634277\n",
            "Epoch [310/1000], Loss: 1.892946720123291\n",
            "Epoch [311/1000], Loss: 1.8716812133789062\n",
            "Epoch [312/1000], Loss: 1.8843605518341064\n",
            "Epoch [313/1000], Loss: 1.883949637413025\n",
            "Epoch [314/1000], Loss: 1.923187255859375\n",
            "Epoch [315/1000], Loss: 1.9945651292800903\n",
            "Epoch [316/1000], Loss: 1.971110224723816\n",
            "Epoch [317/1000], Loss: 1.9743943214416504\n",
            "Epoch [318/1000], Loss: 1.9538846015930176\n",
            "Epoch [319/1000], Loss: 1.879747748374939\n",
            "Epoch [320/1000], Loss: 2.0080151557922363\n",
            "Epoch [321/1000], Loss: 1.922482967376709\n",
            "Epoch [322/1000], Loss: 1.9564354419708252\n",
            "Epoch [323/1000], Loss: 1.912164568901062\n",
            "Epoch [324/1000], Loss: 2.0204949378967285\n",
            "Epoch [325/1000], Loss: 1.976449728012085\n",
            "Epoch [326/1000], Loss: 1.9847161769866943\n",
            "Epoch [327/1000], Loss: 1.914645791053772\n",
            "Epoch [328/1000], Loss: 1.967966914176941\n",
            "Epoch [329/1000], Loss: 1.9603086709976196\n",
            "Epoch [330/1000], Loss: 1.9207236766815186\n",
            "Epoch [331/1000], Loss: 1.886685848236084\n",
            "Epoch [332/1000], Loss: 1.9589611291885376\n",
            "Epoch [333/1000], Loss: 1.9361454248428345\n",
            "Epoch [334/1000], Loss: 1.880217432975769\n",
            "Epoch [335/1000], Loss: 1.898926854133606\n",
            "Epoch [336/1000], Loss: 1.8999571800231934\n",
            "Epoch [337/1000], Loss: 1.9133012294769287\n",
            "Epoch [338/1000], Loss: 1.8818447589874268\n",
            "Epoch [339/1000], Loss: 1.8766402006149292\n",
            "Epoch [340/1000], Loss: 1.940574288368225\n",
            "Epoch [341/1000], Loss: 1.9140369892120361\n",
            "Epoch [342/1000], Loss: 1.9597511291503906\n",
            "Epoch [343/1000], Loss: 1.8550195693969727\n",
            "Epoch [344/1000], Loss: 1.920634150505066\n",
            "Epoch [345/1000], Loss: 1.9322646856307983\n",
            "Epoch [346/1000], Loss: 1.8966515064239502\n",
            "Epoch [347/1000], Loss: 1.8701989650726318\n",
            "Epoch [348/1000], Loss: 1.9218906164169312\n",
            "Epoch [349/1000], Loss: 1.8604322671890259\n",
            "Epoch [350/1000], Loss: 1.8904502391815186\n",
            "Epoch [351/1000], Loss: 1.894315481185913\n",
            "Epoch [352/1000], Loss: 1.8424609899520874\n",
            "Epoch [353/1000], Loss: 1.9089738130569458\n",
            "Epoch [354/1000], Loss: 1.8857951164245605\n",
            "Epoch [355/1000], Loss: 1.8815311193466187\n",
            "Epoch [356/1000], Loss: 1.8702366352081299\n",
            "Epoch [357/1000], Loss: 1.8483542203903198\n",
            "Epoch [358/1000], Loss: 1.8440099954605103\n",
            "Epoch [359/1000], Loss: 1.8776767253875732\n",
            "Epoch [360/1000], Loss: 1.96108078956604\n",
            "Epoch [361/1000], Loss: 1.9066630601882935\n",
            "Epoch [362/1000], Loss: 1.833297848701477\n",
            "Epoch [363/1000], Loss: 1.832563877105713\n",
            "Epoch [364/1000], Loss: 1.8609846830368042\n",
            "Epoch [365/1000], Loss: 1.9419201612472534\n",
            "Epoch [366/1000], Loss: 1.8982536792755127\n",
            "Epoch [367/1000], Loss: 1.921900749206543\n",
            "Epoch [368/1000], Loss: 1.9628243446350098\n",
            "Epoch [369/1000], Loss: 1.914291262626648\n",
            "Epoch [370/1000], Loss: 1.886756420135498\n",
            "Epoch [371/1000], Loss: 1.9268842935562134\n",
            "Epoch [372/1000], Loss: 1.8540000915527344\n",
            "Epoch [373/1000], Loss: 1.8751314878463745\n",
            "Epoch [374/1000], Loss: 1.9024642705917358\n",
            "Epoch [375/1000], Loss: 1.9693301916122437\n",
            "Epoch [376/1000], Loss: 1.8451144695281982\n",
            "Epoch [377/1000], Loss: 1.8423147201538086\n",
            "Epoch [378/1000], Loss: 1.91936457157135\n",
            "Epoch [379/1000], Loss: 1.833320140838623\n",
            "Epoch [380/1000], Loss: 1.9150176048278809\n",
            "Epoch [381/1000], Loss: 1.8475444316864014\n",
            "Epoch [382/1000], Loss: 1.9391794204711914\n",
            "Epoch [383/1000], Loss: 1.8469743728637695\n",
            "Epoch [384/1000], Loss: 1.820897102355957\n",
            "Epoch [385/1000], Loss: 1.8854148387908936\n",
            "Epoch [386/1000], Loss: 1.9245383739471436\n",
            "Epoch [387/1000], Loss: 1.9007536172866821\n",
            "Epoch [388/1000], Loss: 1.8869194984436035\n",
            "Epoch [389/1000], Loss: 1.8211071491241455\n",
            "Epoch [390/1000], Loss: 1.8576081991195679\n",
            "Epoch [391/1000], Loss: 1.9535273313522339\n",
            "Epoch [392/1000], Loss: 1.8229228258132935\n",
            "Epoch [393/1000], Loss: 1.8801482915878296\n",
            "Epoch [394/1000], Loss: 1.9112460613250732\n",
            "Epoch [395/1000], Loss: 1.8965107202529907\n",
            "Epoch [396/1000], Loss: 1.9120941162109375\n",
            "Epoch [397/1000], Loss: 1.84880793094635\n",
            "Epoch [398/1000], Loss: 1.9160577058792114\n",
            "Epoch [399/1000], Loss: 1.898829698562622\n",
            "Epoch [400/1000], Loss: 1.8447529077529907\n",
            "Epoch [401/1000], Loss: 1.8349320888519287\n",
            "Epoch [402/1000], Loss: 1.844153642654419\n",
            "Epoch [403/1000], Loss: 1.8462727069854736\n",
            "Epoch [404/1000], Loss: 1.9047235250473022\n",
            "Epoch [405/1000], Loss: 1.868242621421814\n",
            "Epoch [406/1000], Loss: 1.882333517074585\n",
            "Epoch [407/1000], Loss: 1.8252912759780884\n",
            "Epoch [408/1000], Loss: 1.8573997020721436\n",
            "Epoch [409/1000], Loss: 1.9184050559997559\n",
            "Epoch [410/1000], Loss: 1.8188292980194092\n",
            "Epoch [411/1000], Loss: 1.8832107782363892\n",
            "Epoch [412/1000], Loss: 1.8393583297729492\n",
            "Epoch [413/1000], Loss: 1.8713080883026123\n",
            "Epoch [414/1000], Loss: 1.9604411125183105\n",
            "Epoch [415/1000], Loss: 1.9257338047027588\n",
            "Epoch [416/1000], Loss: 1.8084228038787842\n",
            "Epoch [417/1000], Loss: 1.8233484029769897\n",
            "Epoch [418/1000], Loss: 1.8663029670715332\n",
            "Epoch [419/1000], Loss: 1.8700538873672485\n",
            "Epoch [420/1000], Loss: 1.800175428390503\n",
            "Epoch [421/1000], Loss: 1.8273817300796509\n",
            "Epoch [422/1000], Loss: 1.8363053798675537\n",
            "Epoch [423/1000], Loss: 1.8969780206680298\n",
            "Epoch [424/1000], Loss: 1.8869869709014893\n",
            "Epoch [425/1000], Loss: 1.8134804964065552\n",
            "Epoch [426/1000], Loss: 1.8869792222976685\n",
            "Epoch [427/1000], Loss: 1.8052618503570557\n",
            "Epoch [428/1000], Loss: 1.8111158609390259\n",
            "Epoch [429/1000], Loss: 1.9285334348678589\n",
            "Epoch [430/1000], Loss: 1.8387367725372314\n",
            "Epoch [431/1000], Loss: 1.8516765832901\n",
            "Epoch [432/1000], Loss: 1.8596093654632568\n",
            "Epoch [433/1000], Loss: 1.9094433784484863\n",
            "Epoch [434/1000], Loss: 1.7959219217300415\n",
            "Epoch [435/1000], Loss: 1.8993819952011108\n",
            "Epoch [436/1000], Loss: 1.8711727857589722\n",
            "Epoch [437/1000], Loss: 1.7900629043579102\n",
            "Epoch [438/1000], Loss: 1.7864302396774292\n",
            "Epoch [439/1000], Loss: 1.8387774229049683\n",
            "Epoch [440/1000], Loss: 1.9205642938613892\n",
            "Epoch [441/1000], Loss: 1.832358956336975\n",
            "Epoch [442/1000], Loss: 1.8379117250442505\n",
            "Epoch [443/1000], Loss: 1.8383352756500244\n",
            "Epoch [444/1000], Loss: 1.789993405342102\n",
            "Epoch [445/1000], Loss: 1.8250586986541748\n",
            "Epoch [446/1000], Loss: 1.8608092069625854\n",
            "Epoch [447/1000], Loss: 1.782220721244812\n",
            "Epoch [448/1000], Loss: 1.8528847694396973\n",
            "Epoch [449/1000], Loss: 1.8166368007659912\n",
            "Epoch [450/1000], Loss: 1.853704810142517\n",
            "Epoch [451/1000], Loss: 1.8600999116897583\n",
            "Epoch [452/1000], Loss: 1.7807159423828125\n",
            "Epoch [453/1000], Loss: 1.8172649145126343\n",
            "Epoch [454/1000], Loss: 1.8004276752471924\n",
            "Epoch [455/1000], Loss: 1.855983018875122\n",
            "Epoch [456/1000], Loss: 1.8066987991333008\n",
            "Epoch [457/1000], Loss: 1.8477340936660767\n",
            "Epoch [458/1000], Loss: 1.7625813484191895\n",
            "Epoch [459/1000], Loss: 1.8535614013671875\n",
            "Epoch [460/1000], Loss: 1.8394732475280762\n",
            "Epoch [461/1000], Loss: 1.8120652437210083\n",
            "Epoch [462/1000], Loss: 1.7997246980667114\n",
            "Epoch [463/1000], Loss: 1.8487991094589233\n",
            "Epoch [464/1000], Loss: 1.8010847568511963\n",
            "Epoch [465/1000], Loss: 1.8453007936477661\n",
            "Epoch [466/1000], Loss: 1.826291799545288\n",
            "Epoch [467/1000], Loss: 1.806908130645752\n",
            "Epoch [468/1000], Loss: 1.7902220487594604\n",
            "Epoch [469/1000], Loss: 1.8214542865753174\n",
            "Epoch [470/1000], Loss: 1.772781491279602\n",
            "Epoch [471/1000], Loss: 1.8659476041793823\n",
            "Epoch [472/1000], Loss: 1.775305151939392\n",
            "Epoch [473/1000], Loss: 1.8146424293518066\n",
            "Epoch [474/1000], Loss: 1.7988122701644897\n",
            "Epoch [475/1000], Loss: 1.7686855792999268\n",
            "Epoch [476/1000], Loss: 1.8003168106079102\n",
            "Epoch [477/1000], Loss: 1.857934832572937\n",
            "Epoch [478/1000], Loss: 1.7608696222305298\n",
            "Epoch [479/1000], Loss: 1.8151696920394897\n",
            "Epoch [480/1000], Loss: 1.7900869846343994\n",
            "Epoch [481/1000], Loss: 1.839049220085144\n",
            "Epoch [482/1000], Loss: 1.8540403842926025\n",
            "Epoch [483/1000], Loss: 1.9228923320770264\n",
            "Epoch [484/1000], Loss: 1.9034773111343384\n",
            "Epoch [485/1000], Loss: 1.8364934921264648\n",
            "Epoch [486/1000], Loss: 1.81764554977417\n",
            "Epoch [487/1000], Loss: 1.805587887763977\n",
            "Epoch [488/1000], Loss: 1.9071710109710693\n",
            "Epoch [489/1000], Loss: 1.8446364402770996\n",
            "Epoch [490/1000], Loss: 1.8725662231445312\n",
            "Epoch [491/1000], Loss: 1.807907223701477\n",
            "Epoch [492/1000], Loss: 1.8312257528305054\n",
            "Epoch [493/1000], Loss: 1.768731713294983\n",
            "Epoch [494/1000], Loss: 1.8872861862182617\n",
            "Epoch [495/1000], Loss: 1.7960280179977417\n",
            "Epoch [496/1000], Loss: 1.7704360485076904\n",
            "Epoch [497/1000], Loss: 1.7970688343048096\n",
            "Epoch [498/1000], Loss: 1.7924766540527344\n",
            "Epoch [499/1000], Loss: 1.8632605075836182\n",
            "Epoch [500/1000], Loss: 1.836304783821106\n",
            "Epoch [501/1000], Loss: 1.7552005052566528\n",
            "Epoch [502/1000], Loss: 1.858787178993225\n",
            "Epoch [503/1000], Loss: 1.8407235145568848\n",
            "Epoch [504/1000], Loss: 1.786881923675537\n",
            "Epoch [505/1000], Loss: 1.794134497642517\n",
            "Epoch [506/1000], Loss: 1.7520796060562134\n",
            "Epoch [507/1000], Loss: 1.7903982400894165\n",
            "Epoch [508/1000], Loss: 1.7854350805282593\n",
            "Epoch [509/1000], Loss: 1.7512882947921753\n",
            "Epoch [510/1000], Loss: 1.7455735206604004\n",
            "Epoch [511/1000], Loss: 1.7398563623428345\n",
            "Epoch [512/1000], Loss: 1.8087249994277954\n",
            "Epoch [513/1000], Loss: 1.8192763328552246\n",
            "Epoch [514/1000], Loss: 1.766372799873352\n",
            "Epoch [515/1000], Loss: 1.7309718132019043\n",
            "Epoch [516/1000], Loss: 1.747860312461853\n",
            "Epoch [517/1000], Loss: 1.7804902791976929\n",
            "Epoch [518/1000], Loss: 1.8070917129516602\n",
            "Epoch [519/1000], Loss: 1.7422350645065308\n",
            "Epoch [520/1000], Loss: 1.8122978210449219\n",
            "Epoch [521/1000], Loss: 1.790232539176941\n",
            "Epoch [522/1000], Loss: 1.8527650833129883\n",
            "Epoch [523/1000], Loss: 1.8060369491577148\n",
            "Epoch [524/1000], Loss: 1.776076316833496\n",
            "Epoch [525/1000], Loss: 1.7984223365783691\n",
            "Epoch [526/1000], Loss: 1.7372828722000122\n",
            "Epoch [527/1000], Loss: 1.7402862310409546\n",
            "Epoch [528/1000], Loss: 1.7663497924804688\n",
            "Epoch [529/1000], Loss: 1.9051780700683594\n",
            "Epoch [530/1000], Loss: 1.716575026512146\n",
            "Epoch [531/1000], Loss: 1.7836390733718872\n",
            "Epoch [532/1000], Loss: 1.910706639289856\n",
            "Epoch [533/1000], Loss: 1.7722866535186768\n",
            "Epoch [534/1000], Loss: 1.8928261995315552\n",
            "Epoch [535/1000], Loss: 1.74431574344635\n",
            "Epoch [536/1000], Loss: 1.7494943141937256\n",
            "Epoch [537/1000], Loss: 1.7660644054412842\n",
            "Epoch [538/1000], Loss: 1.7420589923858643\n",
            "Epoch [539/1000], Loss: 1.8549237251281738\n",
            "Epoch [540/1000], Loss: 1.8234367370605469\n",
            "Epoch [541/1000], Loss: 1.7815368175506592\n",
            "Epoch [542/1000], Loss: 1.7540017366409302\n",
            "Epoch [543/1000], Loss: 1.7351189851760864\n",
            "Epoch [544/1000], Loss: 1.765657901763916\n",
            "Epoch [545/1000], Loss: 1.7469505071640015\n",
            "Epoch [546/1000], Loss: 1.7480157613754272\n",
            "Epoch [547/1000], Loss: 1.8034721612930298\n",
            "Epoch [548/1000], Loss: 1.872521162033081\n",
            "Epoch [549/1000], Loss: 1.716927170753479\n",
            "Epoch [550/1000], Loss: 1.7739955186843872\n",
            "Epoch [551/1000], Loss: 1.7608895301818848\n",
            "Epoch [552/1000], Loss: 1.8011236190795898\n",
            "Epoch [553/1000], Loss: 1.710512399673462\n",
            "Epoch [554/1000], Loss: 1.7942614555358887\n",
            "Epoch [555/1000], Loss: 1.8510785102844238\n",
            "Epoch [556/1000], Loss: 1.7320563793182373\n",
            "Epoch [557/1000], Loss: 1.8657678365707397\n",
            "Epoch [558/1000], Loss: 1.7277449369430542\n",
            "Epoch [559/1000], Loss: 1.706494927406311\n",
            "Epoch [560/1000], Loss: 1.7074273824691772\n",
            "Epoch [561/1000], Loss: 1.7823503017425537\n",
            "Epoch [562/1000], Loss: 1.8494189977645874\n",
            "Epoch [563/1000], Loss: 1.8393460512161255\n",
            "Epoch [564/1000], Loss: 1.7855360507965088\n",
            "Epoch [565/1000], Loss: 1.7015600204467773\n",
            "Epoch [566/1000], Loss: 1.783469796180725\n",
            "Epoch [567/1000], Loss: 1.798754096031189\n",
            "Epoch [568/1000], Loss: 1.8728535175323486\n",
            "Epoch [569/1000], Loss: 1.7940952777862549\n",
            "Epoch [570/1000], Loss: 1.8365200757980347\n",
            "Epoch [571/1000], Loss: 1.8154082298278809\n",
            "Epoch [572/1000], Loss: 1.780206561088562\n",
            "Epoch [573/1000], Loss: 1.7888398170471191\n",
            "Epoch [574/1000], Loss: 1.8491243124008179\n",
            "Epoch [575/1000], Loss: 1.8573570251464844\n",
            "Epoch [576/1000], Loss: 1.8502721786499023\n",
            "Epoch [577/1000], Loss: 1.7151745557785034\n",
            "Epoch [578/1000], Loss: 1.7309297323226929\n",
            "Epoch [579/1000], Loss: 1.7081694602966309\n",
            "Epoch [580/1000], Loss: 1.708794116973877\n",
            "Epoch [581/1000], Loss: 1.7927387952804565\n",
            "Epoch [582/1000], Loss: 1.7880326509475708\n",
            "Epoch [583/1000], Loss: 1.8281583786010742\n",
            "Epoch [584/1000], Loss: 1.7950263023376465\n",
            "Epoch [585/1000], Loss: 1.7036288976669312\n",
            "Epoch [586/1000], Loss: 1.7222685813903809\n",
            "Epoch [587/1000], Loss: 1.733213186264038\n",
            "Epoch [588/1000], Loss: 1.7822010517120361\n",
            "Epoch [589/1000], Loss: 1.7661494016647339\n",
            "Epoch [590/1000], Loss: 1.8610045909881592\n",
            "Epoch [591/1000], Loss: 1.6991751194000244\n",
            "Epoch [592/1000], Loss: 1.8085216283798218\n",
            "Epoch [593/1000], Loss: 1.7424589395523071\n",
            "Epoch [594/1000], Loss: 1.7796326875686646\n",
            "Epoch [595/1000], Loss: 1.788151741027832\n",
            "Epoch [596/1000], Loss: 1.7744393348693848\n",
            "Epoch [597/1000], Loss: 1.7436796426773071\n",
            "Epoch [598/1000], Loss: 1.7555433511734009\n",
            "Epoch [599/1000], Loss: 1.7734309434890747\n",
            "Epoch [600/1000], Loss: 1.7400355339050293\n",
            "Epoch [601/1000], Loss: 1.8172852993011475\n",
            "Epoch [602/1000], Loss: 1.6925950050354004\n",
            "Epoch [603/1000], Loss: 1.7984439134597778\n",
            "Epoch [604/1000], Loss: 1.812731146812439\n",
            "Epoch [605/1000], Loss: 1.8038996458053589\n",
            "Epoch [606/1000], Loss: 1.6986318826675415\n",
            "Epoch [607/1000], Loss: 1.7252763509750366\n",
            "Epoch [608/1000], Loss: 1.7457644939422607\n",
            "Epoch [609/1000], Loss: 1.753361463546753\n",
            "Epoch [610/1000], Loss: 1.757829189300537\n",
            "Epoch [611/1000], Loss: 1.8018198013305664\n",
            "Epoch [612/1000], Loss: 1.7562390565872192\n",
            "Epoch [613/1000], Loss: 1.6992051601409912\n",
            "Epoch [614/1000], Loss: 1.749136209487915\n",
            "Epoch [615/1000], Loss: 1.768923044204712\n",
            "Epoch [616/1000], Loss: 1.718906044960022\n",
            "Epoch [617/1000], Loss: 1.6956706047058105\n",
            "Epoch [618/1000], Loss: 1.791122317314148\n",
            "Epoch [619/1000], Loss: 1.7667509317398071\n",
            "Epoch [620/1000], Loss: 1.7629175186157227\n",
            "Epoch [621/1000], Loss: 1.759993314743042\n",
            "Epoch [622/1000], Loss: 1.858686923980713\n",
            "Epoch [623/1000], Loss: 1.7506184577941895\n",
            "Epoch [624/1000], Loss: 1.7091902494430542\n",
            "Epoch [625/1000], Loss: 1.842331051826477\n",
            "Epoch [626/1000], Loss: 1.7653862237930298\n",
            "Epoch [627/1000], Loss: 1.8021605014801025\n",
            "Epoch [628/1000], Loss: 1.7335284948349\n",
            "Epoch [629/1000], Loss: 1.7913001775741577\n",
            "Epoch [630/1000], Loss: 1.7893062829971313\n",
            "Epoch [631/1000], Loss: 1.7147291898727417\n",
            "Epoch [632/1000], Loss: 1.7734397649765015\n",
            "Epoch [633/1000], Loss: 1.701810359954834\n",
            "Epoch [634/1000], Loss: 1.7722573280334473\n",
            "Epoch [635/1000], Loss: 1.7622413635253906\n",
            "Epoch [636/1000], Loss: 1.809085488319397\n",
            "Epoch [637/1000], Loss: 1.749742031097412\n",
            "Epoch [638/1000], Loss: 1.7542496919631958\n",
            "Epoch [639/1000], Loss: 1.7680325508117676\n",
            "Epoch [640/1000], Loss: 1.727370023727417\n",
            "Epoch [641/1000], Loss: 1.6954978704452515\n",
            "Epoch [642/1000], Loss: 1.7034341096878052\n",
            "Epoch [643/1000], Loss: 1.719753623008728\n",
            "Epoch [644/1000], Loss: 1.7195438146591187\n",
            "Epoch [645/1000], Loss: 1.6832265853881836\n",
            "Epoch [646/1000], Loss: 1.6896286010742188\n",
            "Epoch [647/1000], Loss: 1.7147520780563354\n",
            "Epoch [648/1000], Loss: 1.7747780084609985\n",
            "Epoch [649/1000], Loss: 1.728395700454712\n",
            "Epoch [650/1000], Loss: 1.7512288093566895\n",
            "Epoch [651/1000], Loss: 1.6747974157333374\n",
            "Epoch [652/1000], Loss: 1.7597159147262573\n",
            "Epoch [653/1000], Loss: 1.6759452819824219\n",
            "Epoch [654/1000], Loss: 1.728098750114441\n",
            "Epoch [655/1000], Loss: 1.6683812141418457\n",
            "Epoch [656/1000], Loss: 1.7919998168945312\n",
            "Epoch [657/1000], Loss: 1.6742942333221436\n",
            "Epoch [658/1000], Loss: 1.7667834758758545\n",
            "Epoch [659/1000], Loss: 1.7473863363265991\n",
            "Epoch [660/1000], Loss: 1.7148817777633667\n",
            "Epoch [661/1000], Loss: 1.8107125759124756\n",
            "Epoch [662/1000], Loss: 1.718072772026062\n",
            "Epoch [663/1000], Loss: 1.7313973903656006\n",
            "Epoch [664/1000], Loss: 1.6747745275497437\n",
            "Epoch [665/1000], Loss: 1.6868925094604492\n",
            "Epoch [666/1000], Loss: 1.7285799980163574\n",
            "Epoch [667/1000], Loss: 1.7603062391281128\n",
            "Epoch [668/1000], Loss: 1.7358574867248535\n",
            "Epoch [669/1000], Loss: 1.7173024415969849\n",
            "Epoch [670/1000], Loss: 1.8335529565811157\n",
            "Epoch [671/1000], Loss: 1.7367010116577148\n",
            "Epoch [672/1000], Loss: 1.7094032764434814\n",
            "Epoch [673/1000], Loss: 1.7209863662719727\n",
            "Epoch [674/1000], Loss: 1.7176246643066406\n",
            "Epoch [675/1000], Loss: 1.7499668598175049\n",
            "Epoch [676/1000], Loss: 1.663652777671814\n",
            "Epoch [677/1000], Loss: 1.6829875707626343\n",
            "Epoch [678/1000], Loss: 1.6958781480789185\n",
            "Epoch [679/1000], Loss: 1.6907635927200317\n",
            "Epoch [680/1000], Loss: 1.6700718402862549\n",
            "Epoch [681/1000], Loss: 1.6675204038619995\n",
            "Epoch [682/1000], Loss: 1.757148027420044\n",
            "Epoch [683/1000], Loss: 1.6903738975524902\n",
            "Epoch [684/1000], Loss: 1.6961572170257568\n",
            "Epoch [685/1000], Loss: 1.703798532485962\n",
            "Epoch [686/1000], Loss: 1.6491621732711792\n",
            "Epoch [687/1000], Loss: 1.7223316431045532\n",
            "Epoch [688/1000], Loss: 1.6969505548477173\n",
            "Epoch [689/1000], Loss: 1.69515061378479\n",
            "Epoch [690/1000], Loss: 1.697566032409668\n",
            "Epoch [691/1000], Loss: 1.8430536985397339\n",
            "Epoch [692/1000], Loss: 1.7193989753723145\n",
            "Epoch [693/1000], Loss: 1.8160821199417114\n",
            "Epoch [694/1000], Loss: 1.642134666442871\n",
            "Epoch [695/1000], Loss: 1.671634554862976\n",
            "Epoch [696/1000], Loss: 1.818691611289978\n",
            "Epoch [697/1000], Loss: 1.7467901706695557\n",
            "Epoch [698/1000], Loss: 1.7032734155654907\n",
            "Epoch [699/1000], Loss: 1.763577938079834\n",
            "Epoch [700/1000], Loss: 1.7366937398910522\n",
            "Epoch [701/1000], Loss: 1.6594634056091309\n",
            "Epoch [702/1000], Loss: 1.6604152917861938\n",
            "Epoch [703/1000], Loss: 1.7129687070846558\n",
            "Epoch [704/1000], Loss: 1.7284349203109741\n",
            "Epoch [705/1000], Loss: 1.6426509618759155\n",
            "Epoch [706/1000], Loss: 1.6702052354812622\n",
            "Epoch [707/1000], Loss: 1.6797456741333008\n",
            "Epoch [708/1000], Loss: 1.6559208631515503\n",
            "Epoch [709/1000], Loss: 1.642966628074646\n",
            "Epoch [710/1000], Loss: 1.6420962810516357\n",
            "Epoch [711/1000], Loss: 1.7866886854171753\n",
            "Epoch [712/1000], Loss: 1.6921066045761108\n",
            "Epoch [713/1000], Loss: 1.663519263267517\n",
            "Epoch [714/1000], Loss: 1.742641568183899\n",
            "Epoch [715/1000], Loss: 1.688616156578064\n",
            "Epoch [716/1000], Loss: 1.7005019187927246\n",
            "Epoch [717/1000], Loss: 1.6469022035598755\n",
            "Epoch [718/1000], Loss: 1.6991450786590576\n",
            "Epoch [719/1000], Loss: 1.8563735485076904\n",
            "Epoch [720/1000], Loss: 1.6892294883728027\n",
            "Epoch [721/1000], Loss: 1.793115258216858\n",
            "Epoch [722/1000], Loss: 1.6419329643249512\n",
            "Epoch [723/1000], Loss: 1.683968186378479\n",
            "Epoch [724/1000], Loss: 1.6424065828323364\n",
            "Epoch [725/1000], Loss: 1.7461825609207153\n",
            "Epoch [726/1000], Loss: 1.6443519592285156\n",
            "Epoch [727/1000], Loss: 1.7714636325836182\n",
            "Epoch [728/1000], Loss: 1.7085177898406982\n",
            "Epoch [729/1000], Loss: 1.6916170120239258\n",
            "Epoch [730/1000], Loss: 1.6353447437286377\n",
            "Epoch [731/1000], Loss: 1.7172759771347046\n",
            "Epoch [732/1000], Loss: 1.6643059253692627\n",
            "Epoch [733/1000], Loss: 1.7177362442016602\n",
            "Epoch [734/1000], Loss: 1.6190271377563477\n",
            "Epoch [735/1000], Loss: 1.6750521659851074\n",
            "Epoch [736/1000], Loss: 1.7332240343093872\n",
            "Epoch [737/1000], Loss: 1.6718977689743042\n",
            "Epoch [738/1000], Loss: 1.8000274896621704\n",
            "Epoch [739/1000], Loss: 1.742573857307434\n",
            "Epoch [740/1000], Loss: 1.697278618812561\n",
            "Epoch [741/1000], Loss: 1.710862398147583\n",
            "Epoch [742/1000], Loss: 1.6526376008987427\n",
            "Epoch [743/1000], Loss: 1.7076056003570557\n",
            "Epoch [744/1000], Loss: 1.7793023586273193\n",
            "Epoch [745/1000], Loss: 1.7771716117858887\n",
            "Epoch [746/1000], Loss: 1.6556707620620728\n",
            "Epoch [747/1000], Loss: 1.7205027341842651\n",
            "Epoch [748/1000], Loss: 1.6934467554092407\n",
            "Epoch [749/1000], Loss: 1.7866942882537842\n",
            "Epoch [750/1000], Loss: 1.6514281034469604\n",
            "Epoch [751/1000], Loss: 1.677298903465271\n",
            "Epoch [752/1000], Loss: 1.6958569288253784\n",
            "Epoch [753/1000], Loss: 1.684818148612976\n",
            "Epoch [754/1000], Loss: 1.6944502592086792\n",
            "Epoch [755/1000], Loss: 1.6564031839370728\n",
            "Epoch [756/1000], Loss: 1.6754847764968872\n",
            "Epoch [757/1000], Loss: 1.7771762609481812\n",
            "Epoch [758/1000], Loss: 1.6723947525024414\n",
            "Epoch [759/1000], Loss: 1.660206913948059\n",
            "Epoch [760/1000], Loss: 1.6641933917999268\n",
            "Epoch [761/1000], Loss: 1.7466671466827393\n",
            "Epoch [762/1000], Loss: 1.721975564956665\n",
            "Epoch [763/1000], Loss: 1.666866421699524\n",
            "Epoch [764/1000], Loss: 1.7574105262756348\n",
            "Epoch [765/1000], Loss: 1.716584324836731\n",
            "Epoch [766/1000], Loss: 1.6358674764633179\n",
            "Epoch [767/1000], Loss: 1.6770890951156616\n",
            "Epoch [768/1000], Loss: 1.669671893119812\n",
            "Epoch [769/1000], Loss: 1.6368452310562134\n",
            "Epoch [770/1000], Loss: 1.6605279445648193\n",
            "Epoch [771/1000], Loss: 1.6727138757705688\n",
            "Epoch [772/1000], Loss: 1.6645818948745728\n",
            "Epoch [773/1000], Loss: 1.6333798170089722\n",
            "Epoch [774/1000], Loss: 1.665353536605835\n",
            "Epoch [775/1000], Loss: 1.7391489744186401\n",
            "Epoch [776/1000], Loss: 1.6930556297302246\n",
            "Epoch [777/1000], Loss: 1.7623614072799683\n",
            "Epoch [778/1000], Loss: 1.6959099769592285\n",
            "Epoch [779/1000], Loss: 1.7591983079910278\n",
            "Epoch [780/1000], Loss: 1.774193525314331\n",
            "Epoch [781/1000], Loss: 1.705430507659912\n",
            "Epoch [782/1000], Loss: 1.6522430181503296\n",
            "Epoch [783/1000], Loss: 1.6512274742126465\n",
            "Epoch [784/1000], Loss: 1.6518577337265015\n",
            "Epoch [785/1000], Loss: 1.6722655296325684\n",
            "Epoch [786/1000], Loss: 1.7026584148406982\n",
            "Epoch [787/1000], Loss: 1.6825147867202759\n",
            "Epoch [788/1000], Loss: 1.6220818758010864\n",
            "Epoch [789/1000], Loss: 1.6368231773376465\n",
            "Epoch [790/1000], Loss: 1.626503348350525\n",
            "Epoch [791/1000], Loss: 1.7287272214889526\n",
            "Epoch [792/1000], Loss: 1.6690309047698975\n",
            "Epoch [793/1000], Loss: 1.6501141786575317\n",
            "Epoch [794/1000], Loss: 1.6632952690124512\n",
            "Epoch [795/1000], Loss: 1.62421452999115\n",
            "Epoch [796/1000], Loss: 1.7064229249954224\n",
            "Epoch [797/1000], Loss: 1.723771095275879\n",
            "Epoch [798/1000], Loss: 1.774540662765503\n",
            "Epoch [799/1000], Loss: 1.6158944368362427\n",
            "Epoch [800/1000], Loss: 1.6850578784942627\n",
            "Epoch [801/1000], Loss: 1.6997228860855103\n",
            "Epoch [802/1000], Loss: 1.6557742357254028\n",
            "Epoch [803/1000], Loss: 1.6472070217132568\n",
            "Epoch [804/1000], Loss: 1.6338926553726196\n",
            "Epoch [805/1000], Loss: 1.5979784727096558\n",
            "Epoch [806/1000], Loss: 1.6462249755859375\n",
            "Epoch [807/1000], Loss: 1.6801129579544067\n",
            "Epoch [808/1000], Loss: 1.7041324377059937\n",
            "Epoch [809/1000], Loss: 1.670344591140747\n",
            "Epoch [810/1000], Loss: 1.7853466272354126\n",
            "Epoch [811/1000], Loss: 1.631926417350769\n",
            "Epoch [812/1000], Loss: 1.6591626405715942\n",
            "Epoch [813/1000], Loss: 1.6649774312973022\n",
            "Epoch [814/1000], Loss: 1.648919939994812\n",
            "Epoch [815/1000], Loss: 1.6064761877059937\n",
            "Epoch [816/1000], Loss: 1.611130952835083\n",
            "Epoch [817/1000], Loss: 1.7026971578598022\n",
            "Epoch [818/1000], Loss: 1.6356478929519653\n",
            "Epoch [819/1000], Loss: 1.6049597263336182\n",
            "Epoch [820/1000], Loss: 1.62577486038208\n",
            "Epoch [821/1000], Loss: 1.667068600654602\n",
            "Epoch [822/1000], Loss: 1.653782606124878\n",
            "Epoch [823/1000], Loss: 1.5891234874725342\n",
            "Epoch [824/1000], Loss: 1.592039942741394\n",
            "Epoch [825/1000], Loss: 1.7094354629516602\n",
            "Epoch [826/1000], Loss: 1.650910496711731\n",
            "Epoch [827/1000], Loss: 1.6848300695419312\n",
            "Epoch [828/1000], Loss: 1.6261883974075317\n",
            "Epoch [829/1000], Loss: 1.6743078231811523\n",
            "Epoch [830/1000], Loss: 1.7772201299667358\n",
            "Epoch [831/1000], Loss: 1.590278148651123\n",
            "Epoch [832/1000], Loss: 1.6468167304992676\n",
            "Epoch [833/1000], Loss: 1.5866954326629639\n",
            "Epoch [834/1000], Loss: 1.5927926301956177\n",
            "Epoch [835/1000], Loss: 1.6940982341766357\n",
            "Epoch [836/1000], Loss: 1.6758949756622314\n",
            "Epoch [837/1000], Loss: 1.7852575778961182\n",
            "Epoch [838/1000], Loss: 1.5967285633087158\n",
            "Epoch [839/1000], Loss: 1.679774522781372\n",
            "Epoch [840/1000], Loss: 1.7612327337265015\n",
            "Epoch [841/1000], Loss: 1.7586473226547241\n",
            "Epoch [842/1000], Loss: 1.7031129598617554\n",
            "Epoch [843/1000], Loss: 1.643093228340149\n",
            "Epoch [844/1000], Loss: 1.6265347003936768\n",
            "Epoch [845/1000], Loss: 1.6551741361618042\n",
            "Epoch [846/1000], Loss: 1.6868293285369873\n",
            "Epoch [847/1000], Loss: 1.7153663635253906\n",
            "Epoch [848/1000], Loss: 1.6567535400390625\n",
            "Epoch [849/1000], Loss: 1.6780422925949097\n",
            "Epoch [850/1000], Loss: 1.6775965690612793\n",
            "Epoch [851/1000], Loss: 1.6686062812805176\n",
            "Epoch [852/1000], Loss: 1.6973658800125122\n",
            "Epoch [853/1000], Loss: 1.6534814834594727\n",
            "Epoch [854/1000], Loss: 1.6341400146484375\n",
            "Epoch [855/1000], Loss: 1.702887773513794\n",
            "Epoch [856/1000], Loss: 1.623440146446228\n",
            "Epoch [857/1000], Loss: 1.6420652866363525\n",
            "Epoch [858/1000], Loss: 1.6045151948928833\n",
            "Epoch [859/1000], Loss: 1.6038835048675537\n",
            "Epoch [860/1000], Loss: 1.6130813360214233\n",
            "Epoch [861/1000], Loss: 1.6475142240524292\n",
            "Epoch [862/1000], Loss: 1.5799075365066528\n",
            "Epoch [863/1000], Loss: 1.7065701484680176\n",
            "Epoch [864/1000], Loss: 1.6420036554336548\n",
            "Epoch [865/1000], Loss: 1.7557698488235474\n",
            "Epoch [866/1000], Loss: 1.659461259841919\n",
            "Epoch [867/1000], Loss: 1.5880643129348755\n",
            "Epoch [868/1000], Loss: 1.60869562625885\n",
            "Epoch [869/1000], Loss: 1.6466172933578491\n",
            "Epoch [870/1000], Loss: 1.6194359064102173\n",
            "Epoch [871/1000], Loss: 1.5643877983093262\n",
            "Epoch [872/1000], Loss: 1.7845687866210938\n",
            "Epoch [873/1000], Loss: 1.6906330585479736\n",
            "Epoch [874/1000], Loss: 1.646986961364746\n",
            "Epoch [875/1000], Loss: 1.6693711280822754\n",
            "Epoch [876/1000], Loss: 1.5762747526168823\n",
            "Epoch [877/1000], Loss: 1.6306298971176147\n",
            "Epoch [878/1000], Loss: 1.6595818996429443\n",
            "Epoch [879/1000], Loss: 1.6131205558776855\n",
            "Epoch [880/1000], Loss: 1.5972236394882202\n",
            "Epoch [881/1000], Loss: 1.6132622957229614\n",
            "Epoch [882/1000], Loss: 1.5933383703231812\n",
            "Epoch [883/1000], Loss: 1.643494725227356\n",
            "Epoch [884/1000], Loss: 1.5540026426315308\n",
            "Epoch [885/1000], Loss: 1.6554397344589233\n",
            "Epoch [886/1000], Loss: 1.6411278247833252\n",
            "Epoch [887/1000], Loss: 1.5993010997772217\n",
            "Epoch [888/1000], Loss: 1.7060291767120361\n",
            "Epoch [889/1000], Loss: 1.7601796388626099\n",
            "Epoch [890/1000], Loss: 1.6777781248092651\n",
            "Epoch [891/1000], Loss: 1.5538549423217773\n",
            "Epoch [892/1000], Loss: 1.747983455657959\n",
            "Epoch [893/1000], Loss: 1.6330986022949219\n",
            "Epoch [894/1000], Loss: 1.6353106498718262\n",
            "Epoch [895/1000], Loss: 1.5831342935562134\n",
            "Epoch [896/1000], Loss: 1.6177728176116943\n",
            "Epoch [897/1000], Loss: 1.5665886402130127\n",
            "Epoch [898/1000], Loss: 1.6618216037750244\n",
            "Epoch [899/1000], Loss: 1.603822946548462\n",
            "Epoch [900/1000], Loss: 1.5535714626312256\n",
            "Epoch [901/1000], Loss: 1.7003158330917358\n",
            "Epoch [902/1000], Loss: 1.5565530061721802\n",
            "Epoch [903/1000], Loss: 1.6245055198669434\n",
            "Epoch [904/1000], Loss: 1.5488399267196655\n",
            "Epoch [905/1000], Loss: 1.5386673212051392\n",
            "Epoch [906/1000], Loss: 1.622747540473938\n",
            "Epoch [907/1000], Loss: 1.653567910194397\n",
            "Epoch [908/1000], Loss: 1.6083111763000488\n",
            "Epoch [909/1000], Loss: 1.6344468593597412\n",
            "Epoch [910/1000], Loss: 1.6137471199035645\n",
            "Epoch [911/1000], Loss: 1.614705204963684\n",
            "Epoch [912/1000], Loss: 1.7117375135421753\n",
            "Epoch [913/1000], Loss: 1.579807996749878\n",
            "Epoch [914/1000], Loss: 1.5578908920288086\n",
            "Epoch [915/1000], Loss: 1.556160569190979\n",
            "Epoch [916/1000], Loss: 1.627763032913208\n",
            "Epoch [917/1000], Loss: 1.6251170635223389\n",
            "Epoch [918/1000], Loss: 1.6280958652496338\n",
            "Epoch [919/1000], Loss: 1.7115687131881714\n",
            "Epoch [920/1000], Loss: 1.6251238584518433\n",
            "Epoch [921/1000], Loss: 1.7655047178268433\n",
            "Epoch [922/1000], Loss: 1.5973918437957764\n",
            "Epoch [923/1000], Loss: 1.6666067838668823\n",
            "Epoch [924/1000], Loss: 1.6053493022918701\n",
            "Epoch [925/1000], Loss: 1.541661262512207\n",
            "Epoch [926/1000], Loss: 1.6187727451324463\n",
            "Epoch [927/1000], Loss: 1.5834760665893555\n",
            "Epoch [928/1000], Loss: 1.5367975234985352\n",
            "Epoch [929/1000], Loss: 1.596221923828125\n",
            "Epoch [930/1000], Loss: 1.5697227716445923\n",
            "Epoch [931/1000], Loss: 1.6105172634124756\n",
            "Epoch [932/1000], Loss: 1.6677391529083252\n",
            "Epoch [933/1000], Loss: 1.7085479497909546\n",
            "Epoch [934/1000], Loss: 1.531755805015564\n",
            "Epoch [935/1000], Loss: 1.5412977933883667\n",
            "Epoch [936/1000], Loss: 1.6286448240280151\n",
            "Epoch [937/1000], Loss: 1.6915981769561768\n",
            "Epoch [938/1000], Loss: 1.5914666652679443\n",
            "Epoch [939/1000], Loss: 1.56707763671875\n",
            "Epoch [940/1000], Loss: 1.6543406248092651\n",
            "Epoch [941/1000], Loss: 1.5769867897033691\n",
            "Epoch [942/1000], Loss: 1.6676372289657593\n",
            "Epoch [943/1000], Loss: 1.6732423305511475\n",
            "Epoch [944/1000], Loss: 1.5878126621246338\n",
            "Epoch [945/1000], Loss: 1.544938564300537\n",
            "Epoch [946/1000], Loss: 1.6983506679534912\n",
            "Epoch [947/1000], Loss: 1.5813255310058594\n",
            "Epoch [948/1000], Loss: 1.6315934658050537\n",
            "Epoch [949/1000], Loss: 1.612594723701477\n",
            "Epoch [950/1000], Loss: 1.6067334413528442\n",
            "Epoch [951/1000], Loss: 1.6206241846084595\n",
            "Epoch [952/1000], Loss: 1.6120966672897339\n",
            "Epoch [953/1000], Loss: 1.5482481718063354\n",
            "Epoch [954/1000], Loss: 1.5428361892700195\n",
            "Epoch [955/1000], Loss: 1.5611600875854492\n",
            "Epoch [956/1000], Loss: 1.5656870603561401\n",
            "Epoch [957/1000], Loss: 1.5706186294555664\n",
            "Epoch [958/1000], Loss: 1.6002572774887085\n",
            "Epoch [959/1000], Loss: 1.6811355352401733\n",
            "Epoch [960/1000], Loss: 1.5425500869750977\n",
            "Epoch [961/1000], Loss: 1.571334719657898\n",
            "Epoch [962/1000], Loss: 1.584415316581726\n",
            "Epoch [963/1000], Loss: 1.583072543144226\n",
            "Epoch [964/1000], Loss: 1.6123251914978027\n",
            "Epoch [965/1000], Loss: 1.7339729070663452\n",
            "Epoch [966/1000], Loss: 1.6110073328018188\n",
            "Epoch [967/1000], Loss: 1.5529388189315796\n",
            "Epoch [968/1000], Loss: 1.545868992805481\n",
            "Epoch [969/1000], Loss: 1.733847975730896\n",
            "Epoch [970/1000], Loss: 1.5578023195266724\n",
            "Epoch [971/1000], Loss: 1.5992035865783691\n",
            "Epoch [972/1000], Loss: 1.5382740497589111\n",
            "Epoch [973/1000], Loss: 1.6312432289123535\n",
            "Epoch [974/1000], Loss: 1.6002740859985352\n",
            "Epoch [975/1000], Loss: 1.5675358772277832\n",
            "Epoch [976/1000], Loss: 1.5766711235046387\n",
            "Epoch [977/1000], Loss: 1.616674780845642\n",
            "Epoch [978/1000], Loss: 1.5376631021499634\n",
            "Epoch [979/1000], Loss: 1.6167631149291992\n",
            "Epoch [980/1000], Loss: 1.6805405616760254\n",
            "Epoch [981/1000], Loss: 1.5265575647354126\n",
            "Epoch [982/1000], Loss: 1.5300874710083008\n",
            "Epoch [983/1000], Loss: 1.7600194215774536\n",
            "Epoch [984/1000], Loss: 1.5199813842773438\n",
            "Epoch [985/1000], Loss: 1.6159522533416748\n",
            "Epoch [986/1000], Loss: 1.5860791206359863\n",
            "Epoch [987/1000], Loss: 1.516878604888916\n",
            "Epoch [988/1000], Loss: 1.5030755996704102\n",
            "Epoch [989/1000], Loss: 1.7228996753692627\n",
            "Epoch [990/1000], Loss: 1.5356554985046387\n",
            "Epoch [991/1000], Loss: 1.4921212196350098\n",
            "Epoch [992/1000], Loss: 1.656795620918274\n",
            "Epoch [993/1000], Loss: 1.7067774534225464\n",
            "Epoch [994/1000], Loss: 1.4965434074401855\n",
            "Epoch [995/1000], Loss: 1.5904632806777954\n",
            "Epoch [996/1000], Loss: 1.622186541557312\n",
            "Epoch [997/1000], Loss: 1.5777777433395386\n",
            "Epoch [998/1000], Loss: 1.5607296228408813\n",
            "Epoch [999/1000], Loss: 1.5702940225601196\n",
            "Epoch [1000/1000], Loss: 1.4991892576217651\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "def train(model, num_epochs=100):\n",
        "    global vocab_size, batch_size, device\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        hidden = model.init_hidden(batch_size)  # Initialize hidden state\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0  # Count the number of batches for averaging\n",
        "\n",
        "        for batch in generate_chunk():\n",
        "            batch = torch.tensor(batch, dtype=torch.long).to(device)\n",
        "            inputs, targets = batch[:, :-1], batch[:, 1:]\n",
        "\n",
        "            # Detach hidden state to prevent backprop through entire history\n",
        "            hidden = hidden.detach()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output, hidden = model(inputs, hidden)\n",
        "\n",
        "            # Use reshape instead of view to avoid runtime error\n",
        "            loss = criterion(output.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1  # Increment batch count\n",
        "\n",
        "        # Output average loss for each epoch\n",
        "        avg_loss = epoch_loss / batch_count if batch_count > 0 else float('inf')\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}\")\n",
        "\n",
        "# Run training\n",
        "train(model, num_epochs=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDGW0lmLN9U8"
      },
      "source": [
        "Шаблон функции `generate_sample` также доступен ниже. Вы можете как дозаполнить его, так и написать свою собственную функцию с нуля. Не забывайте, что все примеры в обучающей выборке начинались с токена `<sos>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_tbhUM8QN9U8"
      },
      "outputs": [],
      "source": [
        "def generate_sample(CharRNN, seed_phrase=None, max_length=200, temperature=1.0, device=device):\n",
        "    '''\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n",
        "                        while a smaller temperature converges to the single most likely output\n",
        "    '''\n",
        "    # Initialize the seed sequence\n",
        "    if seed_phrase is not None:\n",
        "        x_sequence = [token_to_idx['<sos>']] + [token_to_idx[token] for token in seed_phrase]\n",
        "    else:\n",
        "        x_sequence = [token_to_idx['<sos>']]\n",
        "\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(device)\n",
        "\n",
        "    # Initialize hidden state\n",
        "    hidden = CharRNN.init_hidden(1)  # Batch size = 1 for generation\n",
        "\n",
        "    # Feed the seed phrase to warm up the hidden state\n",
        "    generated_text = seed_phrase if seed_phrase is not None else ''\n",
        "    for i in range(len(x_sequence[0]) - 1):\n",
        "        _, hidden = CharRNN(x_sequence[:, i].unsqueeze(1), hidden)\n",
        "\n",
        "    # Generate text character by character\n",
        "    for _ in range(max_length - len(generated_text)):\n",
        "        # Forward pass to predict the next character\n",
        "        output, hidden = CharRNN(x_sequence[:, -1].unsqueeze(1), hidden)\n",
        "\n",
        "        # Apply temperature to the output logits and compute probabilities\n",
        "        output = output[:, -1, :] / temperature\n",
        "        probabilities = torch.softmax(output, dim=-1).squeeze()\n",
        "\n",
        "        # Sample from the distribution to pick the next character\n",
        "        next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "        next_char = idx_to_token[next_char_idx]\n",
        "\n",
        "        # Append the predicted character to the generated text\n",
        "        generated_text += next_char\n",
        "\n",
        "        # Update x_sequence with the new character\n",
        "        x_sequence = torch.cat((x_sequence, torch.tensor([[next_char_idx]], dtype=torch.int64).to(device)), dim=1)\n",
        "\n",
        "        # Stop generation if EOS token is encountered\n",
        "        if next_char == '<eos>':\n",
        "            break\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L0kng1EN9U8"
      },
      "source": [
        "Пример текста сгенерированного обученной моделью доступен ниже. Не страшно, что в тексте много несуществующих слов. Используемая модель очень проста: это простая классическая RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GVSNgwhN9U9",
        "outputId": "7e04df88-e5d8-4f96-b91b-594735c61aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " мой дядя самых честных правились,\n",
            "и слава с полновила на старинный\n",
            "и на после как подаренье,\n",
            "и старинный признаний\n",
            "и старый долго пора,\n",
            "давно ли была поразала света\n",
            "приди, свой простите любил без устали всем,\n",
            "который простодушно в ней сердца и потом\n",
            "с ним перестану привычный простодушный,\n",
            "и вот волновала он ленский страсти,\n",
            "в веселить он молвили своей\n",
            "под ним простите не страстью душой\n",
            "и друг не всё разлукал он всех волна,\n",
            "в гостиной старины простотой,\n",
            "и в сем в ней раз меня приности,\n",
            "и слава с\n"
          ]
        }
      ],
      "source": [
        "print(generate_sample(model, ' мой дядя самых честных правил', max_length=500, temperature=0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAyA3nksN9U9"
      },
      "source": [
        "### Сдача задания\n",
        "Сгенерируйте десять последовательностей длиной 500, используя строку ' мой дядя самых честных правил'. Температуру для генерации выберите самостоятельно на основании визуального качества генериуремого текста. Не забудьте удалить все технические токены в случае их наличия.\n",
        "\n",
        "Сгенерированную последовательность сохрание в переменную `generated_phrase` и сдайте сгенерированный ниже файл в контест."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBP3lGgN9U9"
      },
      "outputs": [],
      "source": [
        "seed_phrase = ' мой дядя самых честных правил'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2DZEycJN9U9"
      },
      "outputs": [],
      "source": [
        "generated_phrases = # your code here\n",
        "\n",
        "# For example:\n",
        "\n",
        "# generated_phrases = [\n",
        "#     generate_sample(\n",
        "#         model,\n",
        "#         ' мой дядя самых честных правил',\n",
        "#         max_length=500,\n",
        "#         temperature=1.\n",
        "#     ).replace('<sos>', '')\n",
        "#     for _ in range(10)\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD-P2B3yN9U9"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "import json\n",
        "if 'generated_phrases' not in locals():\n",
        "    raise ValueError(\"Please, save generated phrases to `generated_phrases` variable\")\n",
        "\n",
        "for phrase in generated_phrases:\n",
        "\n",
        "    if not isinstance(phrase, str):\n",
        "        raise ValueError(\"The generated phrase should be a string\")\n",
        "\n",
        "    if len(phrase) != 500:\n",
        "        raise ValueError(\"The `generated_phrase` length should be equal to 500\")\n",
        "\n",
        "    assert all([x in set(tokens) for x in set(list(phrase))]), 'Unknown tokens detected, check your submission!'\n",
        "\n",
        "\n",
        "submission_dict = {\n",
        "    'token_to_idx': token_to_idx,\n",
        "    'generated_phrases': generated_phrases\n",
        "}\n",
        "\n",
        "with open('submission_dict.json', 'w') as iofile:\n",
        "    json.dump(submission_dict, iofile)\n",
        "print('File saved to `submission_dict.json`')\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tCobHKvN9U9"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "NLP HW Lab01_Poetry_generation.v5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}